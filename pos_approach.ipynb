{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook: POS (Part of Speech) approach\n",
    "\n",
    "Use POS to keep only specific verb-noun, adj-noun,... relationships focusing on \"inflation\" as noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\School\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "porter=SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#ECB Color\n",
    "color = (17/255, 49/255, 147/255)\n",
    "\n",
    "\n",
    "# TODO: Adapt the tokenize function to POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice between Stemming or Lemmatization:\n",
    "\n",
    "Stemming: Stemming is generally faster and simpler than lemmatization, but it may not always produce a valid word since it applies simple rules to chop off suffixes. In your case, stemming could potentially match \"beat\" with \"beating\" but may produce incorrect results in some cases.\n",
    "\n",
    "Lemmatization: Lemmatization, on the other hand, maps words to their base or dictionary form, which is more linguistically accurate. It typically requires more computational resources compared to stemming but can provide more accurate results\n",
    "\n",
    "Select modulation for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = 0 means just lowercase, mod = 1 means stemming, mod = 2 means lemmatizing\n",
    "mod = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions:\n",
    "def strip(word):\n",
    "    mod_string = re.sub(r'\\W+', '', word)\n",
    "    return mod_string\n",
    "\n",
    "#the following leaves in place two or more capital letters in a row\n",
    "#will be ignored when using standard stemming\n",
    "def abbr_or_lower(word):\n",
    "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "        return word\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "#modular pipeline for stemming, lemmatizing and lowercasing\n",
    "#note this is NOT lemmatizing using grammar pos \n",
    "def tokenize(text, modulation):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split text into sentences while preserving periods\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if modulation < 2:\n",
    "            # Tokenize and preprocess each sentence\n",
    "            tokens = re.findall(r'\\w+', sentence.lower())\n",
    "            stems = []\n",
    "            for token in tokens:\n",
    "                lowers = abbr_or_lower(token)\n",
    "                if lowers not in stop_words and re.search('[a-zA-Z]', lowers):\n",
    "                    if modulation == 0:\n",
    "                        stems.append(lowers)\n",
    "                    elif modulation == 1:\n",
    "                        stems.append(porter.stem(lowers))\n",
    "            processed_sentence = \" \".join(stems)\n",
    "        else:\n",
    "            doc = sp(sentence)\n",
    "            lemmatized_tokens = []\n",
    "            for token in doc:\n",
    "                if token.text.strip():\n",
    "                    lemmatized_tokens.append(token.lemma_)\n",
    "            processed_sentence = \" \".join(lemmatized_tokens)\n",
    "\n",
    "        processed_sentences.append(processed_sentence)\n",
    "\n",
    "    # Reconstruct the text with preserved sentence boundaries\n",
    "    processed_text = \" \".join(processed_sentences)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def tokenize_word_list(word_list, modulation):\n",
    "    processed_words = []\n",
    "\n",
    "    for word in word_list:\n",
    "        processed_word = word.lower()\n",
    "        if modulation < 2:\n",
    "            if modulation == 0:\n",
    "                processed_words.append(processed_word)\n",
    "            elif modulation == 1:\n",
    "                processed_words.append(porter.stem(processed_word))\n",
    "        else:\n",
    "            # Apply lemmatization using spaCy\n",
    "            doc = sp(word)\n",
    "            lemmatized_text=[]\n",
    "            for w_ in doc:\n",
    "                lemmatized_text.append(w_.lemma_)\n",
    "            processed_words.extend([abbr_or_lower(strip(w)) for w in lemmatized_text if (abbr_or_lower(strip(w))) and (abbr_or_lower(strip(w)) not in stop_words)])\n",
    "\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_end_answer(text):\n",
    "    text = text.replace(r'[end_answer]', '')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters_keep_point(text):\n",
    "    # Define the regex pattern to match special characters except \".\"\n",
    "    pattern = r\"[^\\w\\s\\.\\']\"\n",
    "    # Replace special characters with an empty string\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # Replace consecutive dots with just one dot\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    return text\n",
    "\n",
    "def remove_unnecessary_spaces(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_starting_month(text):\n",
    "    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    pattern = r'^(' + '|'.join(months) + r')\\s'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def regex_matcher(text, as_words):\n",
    "    # Split text into sentences using period as delimiter\n",
    "    sentences = re.split(r'\\.\\s*', text)\n",
    "\n",
    "    selected_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence contains \"inflation\" and any word from as_words\n",
    "        if \"inflation\" in sentence.lower() and any(word in sentence.lower() for word in as_words):\n",
    "            selected_sentences.append(sentence)\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "def regex_matcher_word_order(text, as_words):\n",
    "    # Split text into sentences using period as delimiter\n",
    "    sentences = re.split(r'\\.\\s*', text)\n",
    "\n",
    "    selected_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence contains \"inflation\" and any word from as_words\n",
    "        sentence_lower = sentence.lower()\n",
    "        if any(word in sentence_lower for word in as_words) and \"inflation\" in sentence_lower:\n",
    "            # Split the sentence into words\n",
    "            words = sentence_lower.split()\n",
    "            # Check if any word from as_words appears before \"inflation\"\n",
    "            if any(word in as_words for word in words[:words.index(\"inflation\")]):\n",
    "                selected_sentences.append(sentence)\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "def vectorize(tokens, vocab):\n",
    "    vector=[]\n",
    "    for w in vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def text_length_distribution(df):\n",
    "    df['text_length'] = df['Answers'].apply(len)\n",
    "    plt.figure(dpi=300)\n",
    "    # Plot histogram with a label for the legend\n",
    "    df['text_length'].hist(bins=30, color=color, label='Text Length')\n",
    "    # Add title and labels\n",
    "    plt.title('Distribution of Text Lengths')\n",
    "    plt.xlabel('Text Length')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def pos_tagging(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    # Return part of speech tags\n",
    "    return blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Media</th>\n",
       "      <th>Member</th>\n",
       "      <th>Link</th>\n",
       "      <th>Information</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>Interview with Le Monde</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>https://www.ecb.europa.eu/press/inter/date/202...</td>\n",
       "      <td>Interview with Fabio Panetta, Member of the Ex...</td>\n",
       "      <td>The ECB’s objective is to keep inflation at 2%...</td>\n",
       "      <td>2 June 2023[end_answer] There is no doubt that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>Interview with Les Echos</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>https://www.ecb.europa.eu/press/inter/date/202...</td>\n",
       "      <td>Interview with Fabio Panetta, Member of the Ex...</td>\n",
       "      <td>As the ECB celebrates its 25th anniversary tod...</td>\n",
       "      <td>24 May 2023[end_answer] We are studying the de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2023-05-14</td>\n",
       "      <td>Interview with Il Sole 24 Ore</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>https://www.ecb.europa.eu/press/inter/date/202...</td>\n",
       "      <td>Interview with Luis de Guindos, Vice-President...</td>\n",
       "      <td>With seven hikes in one year, the ECB has rais...</td>\n",
       "      <td>14 May 2023[end_answer] Our mandate is price s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>Interview with Nikkei</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>https://www.ecb.europa.eu/press/inter/date/202...</td>\n",
       "      <td>Interview with Christine Lagarde, President of...</td>\n",
       "      <td>You repeatedly mentioned at the last press con...</td>\n",
       "      <td>10 May 2023[end_answer] There are factors that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>Interview with Le Monde</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>https://www.ecb.europa.eu/press/inter/date/202...</td>\n",
       "      <td>Interview with Philip R. Lane, Member of the E...</td>\n",
       "      <td>Last autumn, a euro area recession at the begi...</td>\n",
       "      <td>25 April 2023[end_answer] Yes, the indicators ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                          Media             Member  \\\n",
       "489 2023-06-02        Interview with Le Monde      Fabio Panetta   \n",
       "490 2023-05-24       Interview with Les Echos      Fabio Panetta   \n",
       "491 2023-05-14  Interview with Il Sole 24 Ore    Luis de Guindos   \n",
       "492 2023-05-10          Interview with Nikkei  Christine Lagarde   \n",
       "493 2023-04-25        Interview with Le Monde     Philip R. Lane   \n",
       "\n",
       "                                                  Link  \\\n",
       "489  https://www.ecb.europa.eu/press/inter/date/202...   \n",
       "490  https://www.ecb.europa.eu/press/inter/date/202...   \n",
       "491  https://www.ecb.europa.eu/press/inter/date/202...   \n",
       "492  https://www.ecb.europa.eu/press/inter/date/202...   \n",
       "493  https://www.ecb.europa.eu/press/inter/date/202...   \n",
       "\n",
       "                                           Information  \\\n",
       "489  Interview with Fabio Panetta, Member of the Ex...   \n",
       "490  Interview with Fabio Panetta, Member of the Ex...   \n",
       "491  Interview with Luis de Guindos, Vice-President...   \n",
       "492  Interview with Christine Lagarde, President of...   \n",
       "493  Interview with Philip R. Lane, Member of the E...   \n",
       "\n",
       "                                             Questions  \\\n",
       "489  The ECB’s objective is to keep inflation at 2%...   \n",
       "490  As the ECB celebrates its 25th anniversary tod...   \n",
       "491  With seven hikes in one year, the ECB has rais...   \n",
       "492  You repeatedly mentioned at the last press con...   \n",
       "493  Last autumn, a euro area recession at the begi...   \n",
       "\n",
       "                                               Answers  \n",
       "489  2 June 2023[end_answer] There is no doubt that...  \n",
       "490  24 May 2023[end_answer] We are studying the de...  \n",
       "491  14 May 2023[end_answer] Our mandate is price s...  \n",
       "492  10 May 2023[end_answer] There are factors that...  \n",
       "493  25 April 2023[end_answer] Yes, the indicators ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset containing the interveiw data\n",
    "df = pd.read_csv('data_complete.csv')\n",
    "\n",
    "# Convert 'date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# For computation speed, limit the data to the last 30 rows:\n",
    "df = df.tail(30)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset before dropping missing values:  (30, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date           0\n",
       "Media          0\n",
       "Member         0\n",
       "Information    0\n",
       "Answers        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We won't need the Questions, Link column for this analysis\n",
    "df = df.drop(columns=['Questions', 'Link'])\n",
    "print(\"Shape of the dataset before dropping missing values: \", df.shape)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset after dropping missing values:  (30, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date           30\n",
       "Media          26\n",
       "Member          6\n",
       "Information    30\n",
       "Answers        30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows with missing values\n",
    "df = df.dropna()\n",
    "print(\"Shape of the dataset after dropping missing values: \", df.shape)\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           datetime64[ns]\n",
       "Media                  object\n",
       "Member                 object\n",
       "Information            object\n",
       "Answers                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#types of data\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_length_distribution(df)\\n# average text length\\nprint(\"The mean length is: \",df[\\'text_length\\'].mean())\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"text_length_distribution(df)\n",
    "# average text length\n",
    "print(\"The mean length is: \",df['text_length'].mean())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list of words to match:  115\n"
     ]
    }
   ],
   "source": [
    "# List of words to match: (taken from paper: Inflation Metaphor in Contemporary American English)\n",
    "as_fire = ['ignite', 'fuel', 'spark', 'dampen', 'stoke', 'kindle', 'choke', 'stifle', 'fan', 'flare-up', 'douse', 'snuff']\n",
    "as_liquids = ['erode', 'surge', 'subside', 'simmers', 'emergence', 'ebb', 'dilute', 'spurt', 'recede', 'buoy']\n",
    "as_plant = ['hedge','nip']\n",
    "as_animal = ['soar', 'runaway', 'rampant', 'curb', 'tame', 'rein', 'spur', 'creep', 'whip', 'gallop', 'halt', 'roar', 'hover', 'curtail', 'spawn', 'gnaw', 'hibernation']\n",
    "as_disease = ['plague', 'worsen', 'benign', 'virulent', 'debilitate', 'bout', 'chronically']\n",
    "as_warfare = ['threat', 'subdue', 'target', 'beat', 'preempt', 'vigilant', 'ravage', 'undermine', 'forestall', 'counteract', 'eliminate', 'ferocious', 'ruinous', 'clobber', 'buster', 'eradicate', 'besieged', 'thwart', 'bash', 'beset']\n",
    "as_sports = ['pace', 'outstrip', 'lag', 'surpass', 'quicken', 'outrun', 'bout', 'best', 'tug-of-war', 'zip', 'overtake']\n",
    "as_machine = ['accelerate', 'faster', 'control', 'slow', 'trigger', 'skyrocket', 'rapid', 'heat', 'escalate', 'decelerate', 'ratchet', 'unchecked', 'readjusted', 'chug', 'damper', 'zoom']\n",
    "as_orientation = ['low', 'rise', 'high', 'increase', 'reduce', 'decline', 'fall', 'exceed', 'spiral', 'hyper', 'upward', 'uptick', 'downward', 'boost', 'peak', 'diminish', 'upturn', 'hike', 'plummet', 'upswing']\n",
    "\n",
    "as_words = as_fire + as_liquids + as_plant + as_animal + as_disease + as_warfare + as_sports + as_machine + as_orientation\n",
    "\n",
    "print(\"Length of the list of words to match: \", len(as_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ignite fuel spark dampen stoke kindle choke stifle fan flare douse snuff erode surge subside simmer emergence ebb dilute spurt recede buoy hedge nip soar runaway rampant curb tame rein spur creep whip gallop halt roar hover curtail spawn gnaw hibernation plague worsen benign virulent debilitate bout chronically threat subdue target beat preempt vigilant ravage undermine forestall counteract eliminate ferocious ruinous clobber buster eradicate besiege thwart bash beset pace outstrip lag surpass quicken outrun bout good tug war zip overtake accelerate fast control slow trigger skyrocket rapid heat escalate decelerate ratchet unchecked readjust chug damper zoom low rise high increase reduce decline fall exceed spiral hyper upward uptick downward boost peak diminish upturn hike plummet upswe'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process the list of words with the same modulation as the interviews\n",
    "as_words = tokenize_word_list(as_words, mod)\n",
    "as_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def preprocess_text(df):\n",
    "        text_preproc = (\n",
    "                df.Answers\n",
    "                .astype(str)\n",
    "                .progress_apply(lambda row: remove_end_answer(row))\n",
    "                .progress_apply(lambda row: remove_numbers(row))\n",
    "                .progress_apply(lambda row: remove_special_characters_keep_point(row))\n",
    "                .progress_apply(lambda row: remove_unnecessary_spaces(row))\n",
    "                .progress_apply(lambda row: remove_starting_month(row))\n",
    "                .progress_apply(lambda row: tokenize(row, mod)))\n",
    "\n",
    "        df[\"Answers_cleaned\"]=text_preproc\n",
    "\n",
    "        return df\n",
    "\n",
    "df = preprocess_text(df)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
